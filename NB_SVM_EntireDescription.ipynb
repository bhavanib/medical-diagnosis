{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing some libraries \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the csv file\n",
    "# make changes according to \n",
    "data=pd.read_csv('C:/Users/Vaibhav Kalakota/Downloads/mtsamples_full_text_to_csv.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample Type</th>\n",
       "      <th>Sample Name</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gastroenterology</td>\n",
       "      <td>Colonoscopy - 19</td>\n",
       "      <td>Colonoscopy with terminal ileum examination. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neurology</td>\n",
       "      <td>TIA - Cosult</td>\n",
       "      <td>A 92-year-old female had a transient episode o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>Lower Extremity Arterial Doppler</td>\n",
       "      <td>Lower Extremity Arterial Doppler\\r (Medical Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Urology</td>\n",
       "      <td>Cystopyelogram - 1</td>\n",
       "      <td>Cystopyelogram, clot evacuation, transurethral...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Orthopedic</td>\n",
       "      <td>Phalanx Amputation</td>\n",
       "      <td>Amputation distal phalanx and partial proximal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Sample Type                       Sample Name  \\\n",
       "0            Gastroenterology                  Colonoscopy - 19   \n",
       "1                   Neurology                      TIA - Cosult   \n",
       "2  Cardiovascular / Pulmonary  Lower Extremity Arterial Doppler   \n",
       "3                     Urology                Cystopyelogram - 1   \n",
       "4                  Orthopedic                Phalanx Amputation   \n",
       "\n",
       "                                         Description  \n",
       "0  Colonoscopy with terminal ileum examination. I...  \n",
       "1  A 92-year-old female had a transient episode o...  \n",
       "2  Lower Extremity Arterial Doppler\\r (Medical Tr...  \n",
       "3  Cystopyelogram, clot evacuation, transurethral...  \n",
       "4  Amputation distal phalanx and partial proximal...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printng the 1st few rows\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['Neurology', 'Orthopedic', 'Ophthalmology', 'Nephrology', 'Obstetrics / Gynecology', 'ENT - Otolaryngology', 'Hematology - Oncology', 'Gastroenterology', 'Cardiovascular / Pulmonary', 'Urology']\n"
     ]
    }
   ],
   "source": [
    "# # a.\tCardiovascular / Pulmonary\n",
    "# b.\tENT - Otolaryngology\n",
    "# c.\tGastroenterology\n",
    "# d.\tHematology - Oncology\n",
    "# e.\tNephrology\n",
    "# f.\tNeurology\n",
    "# g.\tObstetrics / Gynecology\n",
    "# h.\tUrology\n",
    "# i.\tOphthalmology \n",
    "# j.\tOrthopedic \n",
    "\n",
    "\n",
    "# Getting the Total categories\n",
    "Stype=set(data['Sample Type'])\n",
    "# print(Stype)\n",
    "temp=list(Stype)\n",
    "# print(temp)\n",
    "\n",
    "\n",
    "# deleting the types not required\n",
    "temp.remove('Psychiatry / Psychology')\n",
    "temp.remove('Dermatology')\n",
    "# checking the list to know if data removed properly\n",
    "print(len(temp))\n",
    "print(temp)\n",
    "\n",
    "SampleType=temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Sample Type                       Sample Name  \\\n",
      "1            Gastroenterology                  Colonoscopy - 19   \n",
      "2                   Neurology                      TIA - Cosult   \n",
      "3  Cardiovascular / Pulmonary  Lower Extremity Arterial Doppler   \n",
      "4                     Urology                Cystopyelogram - 1   \n",
      "5                  Orthopedic                Phalanx Amputation   \n",
      "\n",
      "                                         Description  \n",
      "1  Colonoscopy with terminal ileum examination. I...  \n",
      "2  A 92-year-old female had a transient episode o...  \n",
      "3  Lower Extremity Arterial Doppler\\r (Medical Tr...  \n",
      "4  Cystopyelogram, clot evacuation, transurethral...  \n",
      "5  Amputation distal phalanx and partial proximal...  \n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe with the required categories only\n",
    "dataset=pd.DataFrame(columns=['Sample Type','Sample Name','Description'])\n",
    "# Link of Interest=https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
    "count=0\n",
    "r=1\n",
    "for i,row in data.iterrows():\n",
    "    if(row['Sample Type'] in SampleType):\n",
    "#         print(list(row))\n",
    "        t=row.to_frame()\n",
    "#         print(type(t))\n",
    "# Link of Interest-https://stackoverflow.com/questions/24284342/insert-a-row-to-pandas-dataframe/24287210\n",
    "        dataset.loc[r]=list(row)\n",
    "        r=r+1\n",
    "print(dataset.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cardiovascular / Pulmonary    371\n",
       "Orthopedic                    355\n",
       "Gastroenterology              230\n",
       "Neurology                     224\n",
       "Urology                       158\n",
       "Obstetrics / Gynecology       153\n",
       "ENT - Otolaryngology           97\n",
       "Hematology - Oncology          90\n",
       "Ophthalmology                  83\n",
       "Nephrology                     78\n",
       "Name: Sample Type, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting occurences of each sample type\n",
    "# Link of Interet-http://cmdlinetips.com/2018/02/how-to-get-frequency-counts-of-a-column-in-pandas-dataframe/\n",
    "dataset['Sample Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for preprocessing the data\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "def cleandata(desc):\n",
    "#     removing the numbers from the text\n",
    "    lettersonly=re.sub(\"[^a-zA-Z]\", \" \", desc)\n",
    "#     Convert to lower case, split into individual words\n",
    "    words = lettersonly.lower().split()                             \n",
    "    \n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abnormal cardiac enzyme profile patient year old gentleman brought emergency room obtundation patient mechanically ventilated originally initial diagnosis septic shock labs showed elevated cardiac enzyme profile medical transcription sample report reason consultation abnormal cardiac enzyme profile history present illness patient year old gentleman brought emergency room obtundation patient mechanically ventilated originally initial diagnosis septic shock labs showed elevated cardiac enzyme profile consultation evaluation patient unable give history history obtained family members per patient son history cardiac disease lives utah presently spending months arizona understand followed physician back utah believes workup done cardiac standpoint negative far prior history chest pain shortness breath per family members coronary risk factors history hypertension history diabetes mellitus ex smoker cholesterol status borderline elevated prior history coronary artery disease family history noncontributory family history nonsignificant surgical history foot surgery per family members medications vitamin supplementation prednisone cyclobenzaprine losartan mg daily nifedipine mg daily lasix potassium supplementation allergies sulfa personal history ex smoker consume alcohol past medical history pulmonary fibrosis prednisone oxygen dependent cellulitis status post foot surgery infection recuperating presentation today respiratory acidosis septicemia septic shock presently mechanical ventilation prior cardiac history elevated cardiac enzyme profile review systems limited physical examination vital signs pulse blood pressure respiratory rate per setting heent atraumatic normocephalic neck supple neck veins flat lungs air entry bilaterally clear rales scattered heart pmi displaced regular systolic murmur grade abdomen soft nontender extremities chronic skin changes markings lower extremities noted pulses found palpable dressing also noted laboratory diagnostic data ekg normal sinus rhythm wide complex labs white count h h platelets inr bun creatinine potassium bicarbonate cardiac enzyme profile troponin total ck myoglobin chest x ray acute changes impression patient year old gentleman pulmonary fibrosis prednisone oxygen dependent respiratory acidosis septicemia septic shock secondary cellulitis leg acute renal shutdown elevated cardiac enzyme profile without prior cardiac history possibly due sepsis also acute renal failure recommendations echocardiogram assess lv function rule cardiac valvular involvement aggressive medical management including dialysis cardiac standpoint conservative treatment juncture cardiac enzyme profile could elevated secondary sepsis also underlying renal failure explained patient family detail regarding condition critical aware\n"
     ]
    }
   ],
   "source": [
    "# print(dataset.head())\n",
    "clean=cleandata(dataset.iloc[0]['Description'])\n",
    "print(clean)\n",
    "\n",
    "# Link of Inteerest-https://stackoverflow.com/questions/33587667/extracting-all-nouns-from-a-text-file-using-nltk\n",
    "# Another idea to try- trimming the strings by getting only the nouns and the verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the entire dataset\n",
    "# print(dataset['Description'].size)\n",
    "cleandesc=[]\n",
    "datasize=dataset['Description'].size\n",
    "for i in range(0, datasize ):\n",
    "    cleandesc.append(cleandata(dataset.iloc[i]['Description']))\n",
    "# print(cleandesc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Sample Type                       Sample Name  \\\n",
      "1            Gastroenterology                  Colonoscopy - 19   \n",
      "2                   Neurology                      TIA - Cosult   \n",
      "3  Cardiovascular / Pulmonary  Lower Extremity Arterial Doppler   \n",
      "4                     Urology                Cystopyelogram - 1   \n",
      "5                  Orthopedic                Phalanx Amputation   \n",
      "\n",
      "                                         Description  \\\n",
      "1  Colonoscopy with terminal ileum examination. I...   \n",
      "2  A 92-year-old female had a transient episode o...   \n",
      "3  Lower Extremity Arterial Doppler\\r (Medical Tr...   \n",
      "4  Cystopyelogram, clot evacuation, transurethral...   \n",
      "5  Amputation distal phalanx and partial proximal...   \n",
      "\n",
      "                                    descwithstemming  \n",
      "1   colonoscopi termin ileum examin iron defici a...  \n",
      "2    year old femal transient episod slur speech ...  \n",
      "3    lower extrem arteri doppler medic transcript...  \n",
      "4    cystopyelogram clot evacu transurethr resect...  \n",
      "5    amput distal phalanx partial proxim phalanx ...  \n"
     ]
    }
   ],
   "source": [
    "# stemming the descriptions and making a new column\n",
    "# stemming the text\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "st=PorterStemmer()\n",
    "s=[]\n",
    "final=\"\"\n",
    "for i in cleandesc:\n",
    "    words=word_tokenize(str(i))\n",
    "    for j in words:\n",
    "        temp=st.stem(j)\n",
    "#         print(type(temp))\n",
    "        final=final+\" \"+temp\n",
    "#     print(final)\n",
    "    s.append(final)\n",
    "    final=\" \"\n",
    "dataset['descwithstemming']=s\n",
    "print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669       abnorm cardiac enzym profil patient year old...\n",
      "328       abnorm echocardiogram find followup short br...\n",
      "1180      abnorm echocardiogram find followup short br...\n",
      "584       abnorm ekg rapid heart rate patient came eme...\n",
      "1061      acut chronic copd exacerb commun acquir pneu...\n",
      "1045      adenosin nuclear scan patient unabl walk tre...\n",
      "1414      adenosin nuclear scan patient unabl walk tre...\n",
      "1154      aortic stenosi insert toronto stentless porc...\n",
      "1076      aortic valv replac use mechan valv two vesse...\n",
      "490       aortogram bilater segment lower extrem run l...\n",
      "298       aortoiliac occlus diseas aortobifemor bypass...\n",
      "889       arteri imag bilater lower extrem medic trans...\n",
      "642       ash split venou port insert right anterior c...\n",
      "352       aspir pneumonia chronic obstruct pulmonari d...\n",
      "1250      atrial fibril rapid ventricular respons wolf...\n",
      "356       atrial fibril short breath patient year old ...\n",
      "619       atyp pneumonia hypoxia rheumatoid arthriti s...\n",
      "1377      bilater carotid cerebr angiogram right femor...\n",
      "1644      bilater carotid ultrasound evalu pain medic ...\n",
      "1603      bilater pleural effus remov bilater french c...\n",
      "918       bronchiol respiratori syncyti viru posit imp...\n",
      "1372      bronchoscopi aspir left upper lobectomi carc...\n",
      "1207      bronchoscopi atelectasi mucou plug medic tra...\n",
      "1707      bronchoscopi bronchoalveolar lavag refractor...\n",
      "983       bronchoscopi brush biopsi persist pneumonia ...\n",
      "154       bronchoscopi brush wash biopsi patient bilat...\n",
      "712       bronchoscopi hypoxia increas pulmonari secre...\n",
      "1768      bronchoscopi persist cough product sputum re...\n",
      "450       bronchoscopi right upper lobe biopsi right u...\n",
      "1686      cardiac catheter coronari arteri diseas plu ...\n",
      "                              ...                        \n",
      "1819      reduct paraphimosi medic transcript sampl re...\n",
      "1282      refractori priapism cavernosaphen shunt pati...\n",
      "1635      releas ventral chorde circumcis repair parti...\n",
      "273       repair left inguin hernia indirect patient s...\n",
      "488       right distal ureter calculu patient hematuri...\n",
      "1753      right hydronephrosi right flank pain atyp dy...\n",
      "1081      right inguin explor left inguin hernia repai...\n",
      "1177      right inguin hernia right direct inguin hern...\n",
      "944       right inguin hernia right inguin hernia repa...\n",
      "390       right lower pole renal stone possibl infect ...\n",
      "1038      right orchiopexi right inguin hernia repair ...\n",
      "419       right undescend testicl orchiopexi herniorrh...\n",
      "1764      right ureteropelv junction obstruct robot as...\n",
      "775       salvag cystectomi difficult due postrad pros...\n",
      "443       solitari left kidney obstruct hypertens chro...\n",
      "567       sparc suburethr sling due stress urinari inc...\n",
      "166       spermatocelectomi orchidopexi medic transcri...\n",
      "382       stage ii neuromodul medic transcript sampl r...\n",
      "1239      transurethr electrosurg resect prostat benig...\n",
      "1729      transurethr resect bladder tumor turbt larg ...\n",
      "333       transurethr resect medium bladder tumor turb...\n",
      "1013      ultrasound examin scrotum due scrotal pain d...\n",
      "611       umbil hernia repair standard curvilinear umb...\n",
      "646       umbil hernia repair templat umbil hernia car...\n",
      "1216      vasectomi year ago fail azoosperm revers two...\n",
      "359       voluntari steril bilater vasectomi va defere...\n",
      "1099      whole bodi radionuclid bone scan due prostat...\n",
      "1404      year old boy histori intermitt swell right i...\n",
      "247       year old male sign symptom benign prostat hy...\n",
      "123       year old man chronic prostat return recheck ...\n",
      "Name: descwithstemming, Length: 1839, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# making a bag of words representation for the stemmed description\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vector=CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,stop_words=None)\n",
    "\n",
    "list1=dataset[:]['descwithstemming']\n",
    "\n",
    "print(list1)\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abnorm cardiac enzym profil year gentleman room mechan ventil diagnosi shock lab show enzym profil transcript report reason consult abnorm enzym profil ill year gentleman room mechan ventil diagnosi shock lab show enzym profil consult histori histori member patient son diseas month arizona physician believ workup standpoint neg chest pain breath famili member coronari risk factor histori diabet mellitu ex smoker cholesterol statu borderlin elev coronari arteri diseas famili histori foot surgeri famili member vitamin supplement prednison cyclobenzaprin mg daili nifedipin mg daili lasix potassium supplement allergi person ex smoker consum alcohol histori pulmonari prednison oxygen cellul statu post foot surgeri recuper present today septicemia shock present mechan ventil histori elev enzym profil review system limit examin sign puls blood pressur respiratori rate heent atraumat neck suppl neck vein lung air bilater rale heart pmi displac systol murmur abdomen skin chang mark note puls palpabl dress diagnost data rhythm lab count h platelet inr bun creatinin potassium bicarbon enzym profil troponin ck myoglobin chest x ray impress year gentleman pulmonari prednison oxygen respiratori acidosi septicemia shock secondari cellul leg elev enzym profil histori possibl sepsi failur echocardiogram assess lv function rule valvular involv aggress manag includ dialysi standpoint conserv treatment junctur enzym profil sepsi failur famili detail regard condit awar\n"
     ]
    }
   ],
   "source": [
    "# Taking only nouns to improve the accuracy here\n",
    "type(list1[0:1])\n",
    "ListofStrings=list1.values.tolist()\n",
    "len(ListofStrings)\n",
    "tokens = nltk.word_tokenize(ListofStrings[0:1][0])\n",
    "tags = nltk.pos_tag(tokens)\n",
    "nouns = [word for word,pos in tags if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "str1 = ' '.join(nouns)\n",
    "print (str1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f=vector.fit_transform(list1)\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array and also they are given as input to SVM\n",
    "train_f=train_f.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1839, 12627)\n"
     ]
    }
   ],
   "source": [
    "# print(vector.get_feature_names())\n",
    "print(train_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n",
      "617        Obstetrics / Gynecology\n",
      "1435    Cardiovascular / Pulmonary\n",
      "Name: Sample Type, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Would be useful for final prediction\n",
    "# size for test, validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# dataset = dataset.sort_values(by=['Sample Type', 'descwithstemming'])\n",
    "dataset = dataset.sort_values(by=['Sample Type', 'descwithstemming'])\n",
    "y=dataset['Sample Type'] \n",
    "x=dataset['descwithstemming']\n",
    "x=vector.fit_transform(x)\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array and also they are given as input to SVM\n",
    "x=x.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "print(X_test[0:1])\n",
    "print(y_test[0:2])\n",
    "print(type(y_test))\n",
    "filenameTrain='X_train.txt';\n",
    "# np.save(filenameTrain, X_test)\n",
    "np.savetxt(filenameTrain, X_test)\n",
    "# X_test.to_csv(filenameTrain)\n",
    "filenameTest='y_test.csv';\n",
    "y_test.to_csv(filenameTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# tf - idf code\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=True)\n",
    "tfidf_train = transformer.fit_transform(X_train).toarray();\n",
    "tfidf_test = transformer.fit_transform(X_test).toarray(); \n",
    "print(tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying PCA on the data\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=300)\n",
    "X_train_PCA=pca.fit_transform(X_train) \n",
    "X_test_PCA=pca.fit_transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9836663193260589\n"
     ]
    }
   ],
   "source": [
    "#PCA coverage\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "X = np.random.randint(5, size=(6, 100))\n",
    "ranger=(X_train.shape[0]-1)/3;\n",
    "X1=X_train[0:400]\n",
    "Y1=y_train[0:400]\n",
    "X2=X_train[401:1201]\n",
    "Y2=y_train[401:1201]\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf2 = MultinomialNB()\n",
    "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "scoresNB_BOW=cross_val_score(clf, X_train, y_train, cv=5) # scores is the accuray array[5]\n",
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "clf1 = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "scoresTF_IDF=cross_val_score(clf,tfidf_train, y_train, cv=5) # scores is the accuray array[5]\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model2.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "clf2 = pickle.load(open(filename, 'rb'))\n",
    "scoresNB_BOW_F1 = cross_val_score(clf1, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "scoresTF_IDF_F1=cross_val_score(clf2,tfidf_train, y_train, cv=5, scoring='f1_macro') \n",
    "scoresNB_BOW_P = cross_val_score(clf1, X_train, y_train, cv=5, scoring='precision_macro')\n",
    "scoresTF_IDF_P=cross_val_score(clf2,tfidf_train, y_train, cv=5, scoring='precision_macro')\n",
    "scoresNB_BOW_R = cross_val_score(clf1, X_train, y_train, cv=5, scoring='recall_macro')\n",
    "scoresTF_IDF_R=cross_val_score(clf2,tfidf_train, y_train, cv=5, scoring='recall_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Cardiovascular / Pulmonary       0.89      0.87      0.88        83\n",
      "                Orthopedic       0.94      0.67      0.78        24\n",
      "          Gastroenterology       0.86      0.88      0.87        49\n",
      "                 Neurology       0.42      0.36      0.38        14\n",
      "                   Urology       0.75      0.50      0.60        12\n",
      "   Obstetrics / Gynecology       0.67      0.83      0.74        46\n",
      "      ENT - Otolaryngology       0.81      0.92      0.86        38\n",
      "     Hematology - Oncology       0.90      1.00      0.95        19\n",
      "             Ophthalmology       0.88      0.86      0.87        81\n",
      "                Nephrology       0.87      0.79      0.83        34\n",
      "\n",
      "               avg / total       0.83      0.83      0.83       400\n",
      "\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Cardiovascular / Pulmonary       0.89      0.87      0.88        83\n",
      "                Orthopedic       0.94      0.67      0.78        24\n",
      "          Gastroenterology       0.86      0.88      0.87        49\n",
      "                 Neurology       0.42      0.36      0.38        14\n",
      "                   Urology       0.75      0.50      0.60        12\n",
      "   Obstetrics / Gynecology       0.67      0.83      0.74        46\n",
      "      ENT - Otolaryngology       0.81      0.92      0.86        38\n",
      "     Hematology - Oncology       0.90      1.00      0.95        19\n",
      "             Ophthalmology       0.88      0.86      0.87        81\n",
      "                Nephrology       0.87      0.79      0.83        34\n",
      "\n",
      "               avg / total       0.83      0.83      0.83       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#metrics for individual classes\n",
    "from sklearn.metrics import classification_report\n",
    "target_names=[\"Cardiovascular / Pulmonary\",\"Orthopedic\",\"Gastroenterology\",\"Neurology\",\"Urology\",\"Obstetrics / Gynecology\",\"ENT - Otolaryngology\",\"Hematology - Oncology\",\"Ophthalmology\",\"Nephrology\"];\n",
    "clf.fit(X2, Y2)\n",
    "print(classification_report(Y1.values.tolist(), clf.predict(X1), target_names=target_names))\n",
    "print(classification_report(Y1.values.tolist(), clf.predict(X1), target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82550336 0.78716216 0.81972789 0.80546075 0.8137931 ]\n",
      "[0.66778523 0.60810811 0.64965986 0.62116041 0.6862069 ]\n",
      "[0.76951565 0.74815455 0.78511405 0.7661628  0.74574989]\n",
      "[0.49551945 0.4299523  0.47656998 0.45728507 0.5128596 ]\n",
      "[0.80180575 0.7801191  0.81795897 0.80077332 0.77839164]\n",
      "[0.57191017 0.56566164 0.57670407 0.65989036 0.58298075]\n",
      "[0.76285589 0.74104455 0.77603598 0.74826564 0.73018341]\n",
      "[0.48823878 0.42493831 0.4663237  0.44601919 0.5063386 ]\n"
     ]
    }
   ],
   "source": [
    "print(scoresNB_BOW)\n",
    "print(scoresTF_IDF)\n",
    "print(scoresNB_BOW_F1)\n",
    "print(scoresTF_IDF_F1)\n",
    "print(scoresNB_BOW_P)\n",
    "print(scoresTF_IDF_P)\n",
    "print(scoresNB_BOW_R)\n",
    "print(scoresTF_IDF_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB for BOW KF1 0.75\n",
      "NB for TF-IDF KF1 0.58\n",
      "NB for BOW KF2 0.9266666666666666\n",
      "NB for TF-IDF KF2 0.6483333333333333\n"
     ]
    }
   ],
   "source": [
    "# Predictions \n",
    "NB_BOW_KF1=np.sum(clf.predict(X2)==Y2.values.tolist())/X2.shape[0];\n",
    "NB_TFIDF_KF1=np.sum(clf2.predict(X2_tf)==Y2.values.tolist())/X2.shape[0];\n",
    "print('NB for BOW KF1',NB_BOW_KF1)\n",
    "print('NB for TF-IDF KF1',NB_TFIDF_KF1)\n",
    "\n",
    "NB_BOW_KF2=np.sum(clf.predict(X4)==Y4.values.tolist())/X2.shape[0];\n",
    "NB_TFIDF_KF2=np.sum(clf2.predict(X4_tf)==Y4.values.tolist())/X2.shape[0];\n",
    "print('NB for BOW KF2',NB_BOW_KF2)\n",
    "print('NB for TF-IDF KF2',NB_TFIDF_KF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM for BOW KF1 0.95875\n",
      "SVM for TF-IDF KF1 0.7175\n",
      "SVM for BOW KF2 0.71\n",
      "SVM for TF-IDF KF2 0.7175\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = OutputCodeClassifier(LinearSVC(random_state=0),code_size=2, random_state=0)\n",
    "SVM_BOW_KF1=np.sum(clf.fit(X_train, y_train).predict(X2)==Y2.values.tolist())/X2.shape[0]\n",
    "# save the model to disk\n",
    "filename = 'finalized_model3.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "clf1 = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "SVM_TFIDF_KF1=np.sum(clf.fit(tfidf_train, y_train).predict(X2_tf)==Y2.values.tolist())/X2.shape[0]\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model4.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "clf2 = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "SVM_BOW_KF2=np.sum(clf1.fit(X_train, y_train).predict(X4)==Y4.values.tolist())/X2.shape[0]\n",
    "SVM_TFIDF_KF1=np.sum(clf2.fit(tfidf_train, y_train).predict(X4_tf)==Y4.values.tolist())/X2.shape[0]\n",
    "scoresSVM_BOW=cross_val_score(clf1, X_train, y_train, cv=5) # for SVM - BOW\n",
    "scoresSVMTF_IDF=cross_val_score(clf2,tfidf_train, y_train, cv=5) # for SVM - TF-IDF\n",
    "scoresSVM_BOW_F1 = cross_val_score(clf1, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "scoresSVMTF_IDF_F1=cross_val_score(clf2,tfidf_train, y_train, cv=5, scoring='f1_macro') \n",
    "scoresSVM_BOW_P = cross_val_score(clf1, X_train, y_train, cv=5, scoring='precision_macro')\n",
    "scoresSVMTF_IDF_P=cross_val_score(clf2,tfidf_train, y_train, cv=5, scoring='precision_macro')\n",
    "scoresSVM_BOW_R = cross_val_score(clf1, X_train, y_train, cv=5, scoring='recall_macro')\n",
    "scoresSVMTF_IDF_R=cross_val_score(clf2,tfidf_train, y_train, cv=5, scoring='recall_macro')\n",
    "\n",
    "print('SVM for BOW KF1',SVM_BOW_KF1)\n",
    "print('SVM for TF-IDF KF1',SVM_TFIDF_KF1)\n",
    "print('SVM for BOW KF2',SVM_BOW_KF2)\n",
    "print('SVM for TF-IDF KF2',SVM_TFIDF_KF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USing Normalizer to processes the inputs i.e Negative values-Postive values -----> (0,1)\n",
    "from sklearn import preprocessing\n",
    "binarizer = preprocessing.Binarizer().fit(X_train_PCA)\n",
    "X_train_PCA_NB=binarizer.transform(X_train_PCA)\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_PCA_NB_MMS=scaler.fit(X_train_PCA).transform(X_train_PCA)\n",
    "\n",
    "X_train_PCA_NB_normalized = preprocessing.normalize(X_train_PCA, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf_NB_PCA = MultinomialNB()\n",
    "scoresNB_BOW_PCA=cross_val_score(clf_NB_PCA, X_train_PCA_NB, y_train, cv=10) # for SVM - BOW\n",
    "scoresSVMTF_IDF_PCA=cross_val_score(clf2,tfidf_train, y_train, cv=10) # for SVM - TF-IDF\n",
    "scoresNB_BOW_F1_PCA = cross_val_score(clf1, X_train_PCA_NB, y_train, cv=10, scoring='f1_macro')\n",
    "scoresTF_IDF_F1_PCA=cross_val_score(clf2,tfidf_train, y_train, cv=10, scoring='f1_macro') \n",
    "scoresNB_BOW_P_PCA = cross_val_score(clf1, X_train_PCA_NB, y_train, cv=10, scoring='precision_macro')\n",
    "scoresTF_IDF_P_PCA=cross_val_score(clf2,tfidf_train, y_train, cv=10, scoring='precision_macro')\n",
    "scoresNB_BOW_R_PCA = cross_val_score(clf1, X_train_PCA_NB, y_train, cv=10, scoring='recall_macro')\n",
    "scoresTF_IDF_R_PCA=cross_val_score(clf2,tfidf_train, y_train, cv=10, scoring='recall_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66887417 0.64238411 0.69536424 0.7114094  0.68243243 0.71428571\n",
      " 0.66438356 0.77083333 0.62937063 0.67375887]\n"
     ]
    }
   ],
   "source": [
    "print(scoresNB_BOW_PCA)\n",
    "print(scoresSVMTF_IDF_PCA)\n",
    "print(scoresNB_BOW_F1_PCA)\n",
    "print(scoresTF_IDF_F1_PCA)\n",
    "print(scoresNB_BOW_P_PCA)\n",
    "print(scoresTF_IDF_P_PCA)\n",
    "print(scoresNB_BOW_R_PCA)\n",
    "print(scoresTF_IDF_R_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM after PCA\n",
    "# print(X_test_PCA.shape)\n",
    "# print(X_train_PCA.shape)\n",
    "scoresSVM_BOW_PCA=cross_val_score(clf1, X_train_PCA, y_train, cv=10) # for SVM - BOW\n",
    "scoresSVMTF_IDF=cross_val_score(clf2,tfidf_train, y_train, cv=10) # for SVM - TF-IDF\n",
    "scoresSVM_BOW_F1 = cross_val_score(clf1, X_train_PCA, y_train, cv=10, scoring='f1_macro')\n",
    "scoresSVMTF_IDF_F1=cross_val_score(clf2,tfidf_train, y_train, cv=10, scoring='f1_macro') \n",
    "scoresSVM_BOW_P = cross_val_score(clf1, X_train_PCA, y_train, cv=10, scoring='precision_macro')\n",
    "scoresSVMTF_IDF_P=cross_val_score(clf2,tfidf_train, y_train, cv=10, scoring='precision_macro')\n",
    "scoresSVM_BOW_R = cross_val_score(clf1, X_train_PCA, y_train, cv=10, scoring='recall_macro')\n",
    "scoresSVMTF_IDF_R=cross_val_score(clf2,tfidf_train, y_train, cv=10, scoring='recall_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74172185 0.78807947 0.8013245  0.7852349  0.79054054 0.80272109\n",
      " 0.80136986 0.83333333 0.75524476 0.78723404]\n"
     ]
    }
   ],
   "source": [
    "print(scoresSVM_BOW_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80536913 0.84459459 0.84067797 0.80479452 0.82413793]\n",
      "[0.82550336 0.85810811 0.84745763 0.83561644 0.84827586]\n",
      "[0.78020652 0.82060228 0.80330176 0.7710854  0.75883854]\n",
      "[0.78117161 0.83324955 0.807953   0.79786262 0.79373705]\n",
      "[0.80404187 0.82992259 0.82066882 0.78744679 0.76375075]\n",
      "[0.82734421 0.84523231 0.83561656 0.81489833 0.80505535]\n",
      "[0.76659255 0.8148934  0.799044   0.76544522 0.75996137]\n",
      "[0.75902099 0.82489589 0.79915629 0.79260162 0.79058297]\n"
     ]
    }
   ],
   "source": [
    "print(scoresSVM_BOW)\n",
    "print(scoresSVMTF_IDF)\n",
    "print(scoresSVM_BOW_F1)\n",
    "print(scoresSVMTF_IDF_F1)\n",
    "print(scoresSVM_BOW_P)\n",
    "print(scoresSVMTF_IDF_P)\n",
    "print(scoresSVM_BOW_R)\n",
    "print(scoresSVMTF_IDF_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN for BOW KF1 0.7875\n"
     ]
    }
   ],
   "source": [
    "# Neural Networks\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-6, hidden_layer_sizes=(20, 20), random_state=1)\n",
    "clf.fit(X1, Y1)\n",
    "print('NN for BOW KF1',np.sum(clf.fit(X1, Y1).predict(X2)==Y2.values.tolist())/X2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "C:/Users/Vaibhav Kalakota/Downloads/X_train.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-193-44b2ef038919>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# outfile='y_train.csv';\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# x_test=np.load(outfile)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mx_testOutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/Vaibhav Kalakota/Downloads/X_train.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0my_testOuput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Vaibhav Kalakota/Downloads/y_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ISO-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# clf2.fit(X1_tf, Y1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    614\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    615\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: C:/Users/Vaibhav Kalakota/Downloads/X_train.txt not found."
     ]
    }
   ],
   "source": [
    "#testing\n",
    "# x_test=pd.read_csv('C:/Users/Vaibhav Kalakota/Downloads/y_train.csv',encoding = \"ISO-8859-1\")\n",
    "# outfile='y_train.csv';\n",
    "# x_test=np.load(outfile)\n",
    "x_testOutput=np.loadtxt(\"C:/Users/Vaibhav Kalakota/Downloads/X_train.txt\")\n",
    "y_testOuput=pd.read_csv('C:/Users/Vaibhav Kalakota/Downloads/y_test.csv',encoding = \"ISO-8859-1\")\n",
    "# clf2.fit(X1_tf, Y1)\n",
    "# # clf3.fit(X1_pca,Y1)\n",
    "# MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "# print(clf.predict(x_testOutput[0:10]))\n",
    "# print(y_testOuput[0:10])\n",
    "# print(np.sum(clf.predict(x_test)==y_test))\n",
    "outPut=clf.predict(x_testOutput);\n",
    "print(outPut[0:10])\n",
    "print(y_testOuput[0:10])\n",
    "print(np.sum(clf.predict(x_testOutput)==y_testOuput.values.tolist()))\n",
    "print('NB for PCA BOW KF1',clf.score(x_testOutput,y_testOuput))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
